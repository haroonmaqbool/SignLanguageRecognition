## Sign Language Recognition System (COMP‚Äë360 Project)
 
 > ‚ÄúCan we make a computer actually *read our hands*?‚Äù  
 
 **Course:** Introduction to Artificial Intelligence (COMP‚Äë360)  
 **Institution:** Forman Christian College  
 **Team:** Haroon ‚Ä¢ Saria ‚Ä¢ Azmeer  
 
 **Idea in one line:**  
 Turn **ASL hand gestures** into **live text and speech** using **MediaPipe**, **CNN models**, and a custom **Flask web app**.
 
 We trained our models on the public **ASL Alphabet Dataset** (A‚ÄìZ hand signs) from **Kaggle**, and then converted each image into hand‚Äëlandmark features using MediaPipe.

---

## What Our Project Can Do

| Feature | Description |
|---------|-------------|
| Real‚Äëtime ASL Detection | Reads your hand signs from a webcam and predicts the current letter. |
| Modern Web UI | Animated landing page and a ‚ÄúReal‚ÄëTime Detection Studio‚Äù for live use. |
| AI Models | CNN‚Äëbased models trained on ASL alphabet landmarks. |
| Sentence Builder | Stable predictions are appended to form full sentences. |
| Text‚Äëto‚ÄëSpeech | One‚Äëclick button to speak out the generated sentence using gTTS. |
| Hand Landmarks | MediaPipe landmarks drawn directly on the video feed for feedback. |

---

## Quick Demo ‚Äî How It Feels to Use

1. Open the web app ‚Üí a **landing page** with an animated hand (`ü§ü`) welcomes you.  
2. Click **‚ÄúTry Now ‚Üí‚Äù** ‚Üí you enter the **Real‚ÄëTime Detection Studio**.  
3. Turn on your webcam ‚Üí the app starts reading your hand signs letter by letter.  
4. The **current letter**, **confidence bar**, and **running sentence** update in real time.  
5. Hit **‚ÄúSpeak Text‚Äù** ‚Üí your sentence is converted to **audio** using gTTS.  

> In simple words: you sign ‚Üí our model predicts ‚Üí the app writes it ‚Üí and then speaks it.

---

## Tech Stack

- **Python 3**
- **Flask** ‚Äì backend web framework
- **TensorFlow / Keras** ‚Äì deep learning models (CNN)
- **MediaPipe Hands** ‚Äì 3D hand landmark detection (21 points)
- **OpenCV** ‚Äì image & video frame handling
- **NumPy, scikit‚Äëlearn** ‚Äì data + evaluation
- **gTTS** ‚Äì Google Text‚Äëto‚ÄëSpeech for audio output
- **HTML / CSS / Vanilla JS** ‚Äì front‚Äëend (all custom, no big CSS framework)

---

## Project Structure (High‚ÄëLevel)

```text
SignLanguageRecognition-SLR/
‚îú‚îÄ‚îÄ app.py                 # Flask web app + real-time detection studio
‚îú‚îÄ‚îÄ preprocessing.py       # ASL dataset preprocessing & landmark extraction
‚îú‚îÄ‚îÄ train_model.py         # CNN model training
‚îú‚îÄ‚îÄ evaluate_model.py      # Model evaluation & plots
‚îú‚îÄ‚îÄ realtime_detection.py  # (Optional) standalone webcam script
‚îú‚îÄ‚îÄ models/                # Trained CNN models (.h5 files)
‚îú‚îÄ‚îÄ processed_data/        # Saved NumPy arrays (X_train, y_train, etc.)
‚îú‚îÄ‚îÄ plots/                 # Training curves & confusion matrices
‚îú‚îÄ‚îÄ reports/               # Classification reports
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ index.html         # Single-page UI (landing + studio)
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

> Note: Some filenames (e.g. model names) may change as we experiment, but the overall structure stays the same.

---

## How to Run the Project

### 1Ô∏è‚É£ Set Up Environment

- Install **Python 3.8+**
- Make sure you have a **webcam** connected

```bash
# (Optional but recommended) create virtual environment
python -m venv slr_env

# Activate (Windows)
slr_env\Scripts\activate

# Activate (macOS / Linux)
source slr_env/bin/activate
```

### 2Ô∏è‚É£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Prepare Data & Train Models (First Time Only)

```bash
# 1. Preprocess ASL dataset (download + landmarks + splits)
python preprocessing.py

# 2. Train CNN model(s)
python train_model.py

# 3. Evaluate and generate plots/reports
python evaluate_model.py
```

Make sure trained models (e.g. `cnn_baseline.h5`, `cnn_last.h5`) are inside the `models/` folder, because `app.py` expects them there.

### 4Ô∏è‚É£ Run the Web App

```bash
python app.py
```

Then open your browser and go to: `http://localhost:5000`

---

## How It Works (Short Version)

- **Step 1 ‚Äì Detect the Hand**  
  We use **MediaPipe Hands** to detect a single hand and extract **21 landmarks** `(x, y, z)` ‚Üí flattened into a **63‚Äëdimensional vector**.

- **Step 2 ‚Äì Normalize & Preprocess**  
  We normalize the landmarks and also **standardize left/right hands** so the model sees a consistent representation.

- **Step 3 ‚Äì CNN Prediction**  
  The 63‚ÄëD vector is passed to a trained **CNN classifier** that outputs probabilities over **26 classes (A‚ÄìZ)**.

- **Step 4 ‚Äì UI Logic**  
  In `app.py`, we:
  - Capture frames from the webcam in the browser
  - Send each frame to `/predict` (Flask route)
  - Draw **landmarks** on top of the image on the server side
  - Send back both **prediction** and **image_with_landmarks** (base64)

- **Step 5 ‚Äì Sentence + Speech**  
  - The front‚Äëend adds stable predictions (seen multiple times) to a running **sentence**  
  - A separate `/text-to-speech` route uses **gTTS** to generate an **MP3** and returns it as base64  
  - The browser plays it directly without saving any files manually

---

## Web App Overview (What We Built in `app.py`)

- **Landing Page**
  - Big animated **ü§ü hand icon**
  - Soft green **particle background** and **grid animation**
  - Our team & course info displayed
  - Four feature cards: Real‚Äëtime, AI‚Äëpowered, Text Generation, High Accuracy

- **Real‚ÄëTime Detection Studio**
  - Live camera feed with **status badge** (`üì∑ Camera Off` / `üî¥ Live`)
  - **Model Selector** dropdown (e.g. `CNN`, `CNN_LAST`)
  - Controls: **Start**, **Stop**, **Clear**
  - Stats: **Letters Detected**, **Words Formed**
  - **Current Gesture** card:
    - Big letter
    - Confidence percentage
    - Animated progress bar
  - **Generated Text** card:
    - Running sentence from your signs
    - **‚ÄúSpeak Text‚Äù** button for TTS

This whole UI is rendered from a single `index.html` file that `app.py` creates in the `templates/` folder if it doesn‚Äôt exist.

---

## Model & Dataset Details

- **Dataset**
   - **Name:** ASL Alphabet Dataset (Kaggle)
   - **Classes:** 26 letters (A‚ÄìZ), each represented by hand‚Äëgesture images
   - Each image is resized and passed through MediaPipe to extract landmarks
  - Data saved as `X_train.npy`, `X_test.npy`, `y_train.npy`, `y_test.npy`

- **Model**
  - Input: 63‚ÄëD landmark vector
  - Architecture:
    - 1D convolution layers + BatchNorm + Dropout
    - Global pooling
    - Dense layers
    - Softmax over 26 classes

- **Evaluation**
  - Accuracy, Precision, Recall, F1‚Äëscore
  - Confusion matrices for each model
  - Training curves (loss & accuracy)

All plots and reports are saved under `plots/` and `reports/`.

---

## Common Issues & Fixes

- **‚ÄúNo trained models found!‚Äù in console**
  - Make sure you ran `train_model.py`
  - Check that `models/cnn_baseline.h5` (or similar) actually exists

- **Webcam not accessible in the browser**
  - Allow camera permissions for `http://localhost:5000`
  - Close other apps using the camera (Zoom, Teams, etc.)

- **Slow performance**
  - Use a smaller webcam resolution
  - Close extra programs
  - (Optional) Use a machine with a GPU for training

---

## Team

- **Haroon** ‚Äì Model integration, backend logic, real‚Äëtime prediction loop  
- **Saria** ‚Äì Dataset preprocessing, experiments, evaluation & reports  
- **Azmeer** ‚Äì Front‚Äëend UI/UX, text‚Äëto‚Äëspeech integration, overall polishing  

*(Roles are approximate; we all helped each other out when things broke.)*

---

## Note

This project was built **for educational purposes** as part of **COMP‚Äë360 (Introduction to Artificial Intelligence)** at **Forman Christian College**.  
