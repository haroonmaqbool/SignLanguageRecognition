## ğŸ‘‹ Sign Language Recognition System (COMPâ€‘360 Project)

> â€œCan we make a computer actually *read our hands*?â€  
This project is our attempt at answering that question using deep learning + computer vision.

**Course:** Introduction to Artificial Intelligence (COMPâ€‘360)  
**Institution:** Forman Christian College  
**Team:** Haroon â€¢ Saria â€¢ Azmeer  
**Instructor:** [Instructor Name]

Our system recognizes **American Sign Language (ASL) alphabet** from hand gestures and turns it into **live text (and speech!)** using a combination of **MediaPipe**, **CNN models**, and a **Flask web app** with a modern UI.

---

## ğŸŒŸ What Our Project Can Do

- **Realâ€‘time ASL Letter Detection** using your webcam  
- **Beautiful Web Interface** with a landing page and â€œRealâ€‘Time Detection Studioâ€  
- **AI Models** (CNN variants) trained on ASL alphabet data  
- **Automatic Sentence Building** from continuous gestures  
- **Textâ€‘toâ€‘Speech**: Speak out the generated sentence with one click  
- **Hand Landmark Visualization** drawn directly on the camera feed  
- **Model Switching**: Choose between different trained CNN models

---

## ğŸ¥ Quick Demo (How It Feels to Use)

1. Open the web app â†’ a **landing page** with an animated hand (`ğŸ¤Ÿ`) welcomes you.  
2. Click **â€œTry Now â†’â€** â†’ you enter the **Realâ€‘Time Detection Studio**.  
3. Turn on your webcam â†’ the app starts reading your hand signs letter by letter.  
4. The **current letter**, **confidence bar**, and **running sentence** update in real time.  
5. Hit **â€œSpeak Textâ€** â†’ your sentence is converted to **audio** using gTTS.  

> In simple words: you sign â†’ our model predicts â†’ the app writes it â†’ and then speaks it.

---

## ğŸ§± Tech Stack (Student Friendly)

- **Python 3**
- **Flask** â€“ backend web framework
- **TensorFlow / Keras** â€“ deep learning models (CNN)
- **MediaPipe Hands** â€“ 3D hand landmark detection (21 points)
- **OpenCV** â€“ image & video frame handling
- **NumPy, scikitâ€‘learn** â€“ data + evaluation
- **gTTS** â€“ Google Textâ€‘toâ€‘Speech for audio output
- **HTML / CSS / Vanilla JS** â€“ frontâ€‘end (all custom, no big CSS framework)

---

## ğŸ“ Project Structure (Highâ€‘Level)

```text
SignLanguageRecognition-SLR/
â”œâ”€â”€ app.py                 # Flask web app + real-time detection studio
â”œâ”€â”€ preprocessing.py       # ASL dataset preprocessing & landmark extraction
â”œâ”€â”€ train_model.py         # CNN model training
â”œâ”€â”€ evaluate_model.py      # Model evaluation & plots
â”œâ”€â”€ realtime_detection.py  # (Optional) standalone webcam script
â”œâ”€â”€ models/                # Trained CNN models (.h5 files)
â”œâ”€â”€ processed_data/        # Saved NumPy arrays (X_train, y_train, etc.)
â”œâ”€â”€ plots/                 # Training curves & confusion matrices
â”œâ”€â”€ reports/               # Classification reports
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html         # Single-page UI (landing + studio)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

> Note: Some filenames (e.g. model names) may change as we experiment, but the overall structure stays the same.

---

## ğŸ”§ How to Run the Project

### 1ï¸âƒ£ Set Up Environment

- Install **Python 3.8+**
- Make sure you have a **webcam** connected

```bash
# (Optional but recommended) create virtual environment
python -m venv slr_env

# Activate (Windows)
slr_env\Scripts\activate

# Activate (macOS / Linux)
source slr_env/bin/activate
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Prepare Data & Train Models (First Time Only)

```bash
# 1. Preprocess ASL dataset (download + landmarks + splits)
python preprocessing.py

# 2. Train CNN model(s)
python train_model.py

# 3. Evaluate and generate plots/reports
python evaluate_model.py
```

Make sure trained models (e.g. `cnn_baseline.h5`, `cnn_last.h5`) are inside the `models/` folder, because `app.py` expects them there.

### 4ï¸âƒ£ Run the Web App

```bash
python app.py
```

Then open your browser and go to: `http://localhost:5000`

---

## ğŸ§  How It Works (Short Version)

- **Step 1 â€“ Detect the Hand**  
  We use **MediaPipe Hands** to detect a single hand and extract **21 landmarks** `(x, y, z)` â†’ flattened into a **63â€‘dimensional vector**.

- **Step 2 â€“ Normalize & Preprocess**  
  We normalize the landmarks and also **standardize left/right hands** so the model sees a consistent representation.

- **Step 3 â€“ CNN Prediction**  
  The 63â€‘D vector is passed to a trained **CNN classifier** that outputs probabilities over **26 classes (Aâ€“Z)**.

- **Step 4 â€“ UI Logic**  
  In `app.py`, we:
  - Capture frames from the webcam in the browser
  - Send each frame to `/predict` (Flask route)
  - Draw **landmarks** on top of the image on the server side
  - Send back both **prediction** and **image_with_landmarks** (base64)

- **Step 5 â€“ Sentence + Speech**  
  - The frontâ€‘end adds stable predictions (seen multiple times) to a running **sentence**  
  - A separate `/text-to-speech` route uses **gTTS** to generate an **MP3** and returns it as base64  
  - The browser plays it directly without saving any files manually

---

## ğŸ’» Web App Overview (What We Built in `app.py`)

- **Landing Page**
  - Big animated **ğŸ¤Ÿ hand icon**
  - Soft green **particle background** and **grid animation**
  - Our team & course info displayed
  - Four feature cards: Realâ€‘time, AIâ€‘powered, Text Generation, High Accuracy

- **Realâ€‘Time Detection Studio**
  - Live camera feed with **status badge** (`ğŸ“· Camera Off` / `ğŸ”´ Live`)
  - **Model Selector** dropdown (e.g. `CNN`, `CNN_LAST`)
  - Controls: **Start**, **Stop**, **Clear**
  - Stats: **Letters Detected**, **Words Formed**
  - **Current Gesture** card:
    - Big letter
    - Confidence percentage
    - Animated progress bar
  - **Generated Text** card:
    - Running sentence from your signs
    - **â€œSpeak Textâ€** button for TTS

This whole UI is rendered from a single `index.html` file that `app.py` creates in the `templates/` folder if it doesnâ€™t exist.

---

## ğŸ“Š Model & Dataset Details

- **Dataset**
  - ASL alphabet dataset (Aâ€“Z)
  - Each image is resized and passed through MediaPipe to extract landmarks
  - Data saved as `X_train.npy`, `X_test.npy`, `y_train.npy`, `y_test.npy`

- **Model**
  - Input: 63â€‘D landmark vector
  - Architecture:
    - 1D convolution layers + BatchNorm + Dropout
    - Global pooling
    - Dense layers
    - Softmax over 26 classes

- **Evaluation**
  - Accuracy, Precision, Recall, F1â€‘score
  - Confusion matrices for each model
  - Training curves (loss & accuracy)

All plots and reports are saved under `plots/` and `reports/`.

---

## ğŸ§ª How We Tested It (Student Perspective)

- Tried different lighting conditions and camera angles  
- Checked **confusing letters** (e.g. â€œMâ€ vs â€œNâ€, or open palm vs â€œBâ€)  
- Verified that the app handles:
  - â€œNo hand detectedâ€ gracefully
  - Very large uploaded images (we resize them serverâ€‘side)
  - Model not found / not loaded

We also added simple **health check** and **model list** endpoints so we can quickly debug whatâ€™s loaded.

---

## ğŸ› Common Issues & Fixes

- **â€œNo trained models found!â€ in console**
  - Make sure you ran `train_model.py`
  - Check that `models/cnn_baseline.h5` (or similar) actually exists

- **Webcam not accessible in the browser**
  - Allow camera permissions for `http://localhost:5000`
  - Close other apps using the camera (Zoom, Teams, etc.)

- **Slow performance**
  - Use a smaller webcam resolution
  - Close extra programs
  - (Optional) Use a machine with a GPU for training

---

## ğŸš€ What We Learned

- How to go from **raw dataset â†’ trained deep learning model â†’ full web app**  
- How **MediaPipe landmarks** simplify the problem compared to raw images  
- Basics of **API design** in Flask (routes like `/predict`, `/models`, `/text-to-speech`)  
- Frontâ€‘end tricks: sending frames from webcam, handling base64 images, and managing state (letters, words, audio)

This was our first time combining **AI + UX/UI + realâ€‘time browser interaction** in one project.

---

## ğŸ¯ Future Work

- Add **Pakistani Sign Language (PSL)** support  
- Move from **letterâ€‘level** to **word/phraseâ€‘level** recognition  
- Add **user accounts** and history of sentences  
- Build a **mobile app** version (possibly with a lightweight model)  
- Improve robustness for different skin tones, backgrounds, and cameras

---

## ğŸ‘¥ Team

- **Haroon** â€“ Model integration, backend logic, realâ€‘time prediction loop  
- **Saria** â€“ Dataset preprocessing, experiments, evaluation & reports  
- **Azmeer** â€“ Frontâ€‘end UI/UX, textâ€‘toâ€‘speech integration, overall polishing  

*(Roles are approximate; we all helped each other out when things broke.)*

---

## ğŸ“Œ Note

This project was built **for educational purposes** as part of **COMPâ€‘360 (Introduction to Artificial Intelligence)** at **Forman Christian College**.  
You are welcome to explore the code, learn from it, and extend it further for your own projects.
